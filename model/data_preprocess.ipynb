{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b64fbba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load CSV file into a DataFrame\n",
    "# Replace 'your_file.csv' with the actual path to your CSV file\n",
    "df = pd.read_csv('C:/Users/dyros/Desktop/dummy_ws/data/AApose10.csv')\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5e4dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_indices = df[df['field.trigger_flag'] == 1].index\n",
    "matching_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642382a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "columns_to_drop = df.columns[0:7]\n",
    "print(df.columns)\n",
    "cols_to_check = ['field.pose_array.poses0.position.x', 'field.pose_array.poses0.position.y',\"field.pose_array.poses0.position.z\"]\n",
    "df = df.drop(columns=columns_to_drop)\n",
    "dup_mask = df.duplicated(subset=cols_to_check, keep=False)\n",
    "\n",
    "# 3) Keep only rows that are unique in the specified columns\n",
    "df = df[~dup_mask].reset_index(drop=True)\n",
    "# Step 3: Check remaining columns\n",
    "\n",
    "# columns_to_drop = df.columns[[1, 3]]  # e.g., ['age', 'email']\n",
    "df\n",
    "# # Step 2: Drop those columns\n",
    "# df = df.drop(columns=columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eba142",
   "metadata": {},
   "outputs": [],
   "source": [
    "matching_indices = df[df['field.trigger_flag'] == 1].index\n",
    "matching_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6da825",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from typing import Union, List\n",
    "import numpy as np\n",
    "\n",
    "def apply_reference_values(\n",
    "    df: pd.DataFrame,\n",
    "    row_start: int,\n",
    "    row_end: int,\n",
    "    col_indices: Union[slice, List[int], tuple],\n",
    "    noise_std: float = 1.0\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Overwrite values in specified column range for rows from row_start to row_end - 1\n",
    "    using the values from row_start as reference.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): the DataFrame to modify\n",
    "    - row_start (int): start index (inclusive)\n",
    "    - row_end (int): end index (exclusive)\n",
    "    - col_indices (slice or list or tuple): column indices to be modified\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: modified DataFrame (copy)\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "\n",
    "    # Convert column indices (int or slice) to actual column names\n",
    "    if isinstance(col_indices, slice):\n",
    "        cols = df.columns[col_indices]\n",
    "    elif isinstance(col_indices, (list, tuple)):\n",
    "        cols = df.columns[list(col_indices)]\n",
    "    else:\n",
    "        raise TypeError(\"col_indices must be a slice, list, or tuple of integers\")\n",
    "\n",
    "    # Get reference values from the start row\n",
    "    reference_values = df.loc[row_start, cols]\n",
    "\n",
    "    # Overwrite target range rows with reference values\n",
    "    noise = np.random.normal(loc=0.0, scale=noise_std, size=df_copy.loc[row_start:row_end - 1, cols].shape)\n",
    "    df_copy.loc[row_start:row_end - 1, cols] = reference_values.values\n",
    "    df_copy.loc[row_start:row_end - 1, cols] = df_copy.loc[row_start:row_end - 1, cols] + noise\n",
    "\n",
    "    return df_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d8579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_1 = 0.5  # for -9:-5\n",
    "std_2 = 1  # for -5:-1\n",
    "\n",
    "new_df = df.copy()\n",
    "\n",
    "for i in range(len(matching_indices) // 2):\n",
    "    row_start = matching_indices[2 * i]\n",
    "    row_end = matching_indices[2 * i + 1]\n",
    "    \n",
    "    # First range: columns -9:-5 with std_1\n",
    "    new_df = apply_reference_values(\n",
    "        new_df,\n",
    "        row_start=row_start,\n",
    "        row_end=row_end,\n",
    "        col_indices=slice(-12, -8),\n",
    "        noise_std=std_1\n",
    "    )\n",
    "    \n",
    "    # Second range: columns -5:-1 with std_2\n",
    "    new_df = apply_reference_values(\n",
    "        new_df,\n",
    "        row_start=row_start,\n",
    "        row_end=row_end,\n",
    "        col_indices=slice(-8, -4),\n",
    "        noise_std=std_2\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709b97fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_csv('modified_output_AA10.csv', index=False, encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e273004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Step 1: 파일 경로 패턴 설정 (예: 'data/file1.csv' ~ 'data/file8.csv')\n",
    "file_paths = [\n",
    "    'modified_output_1.csv',\n",
    "    'modified_output_2.csv',\n",
    "    'modified_output_3.csv',\n",
    "    'modified_output_4.csv',\n",
    "    'modified_output_5.csv',\n",
    "    'modified_output_6.csv',\n",
    "    'modified_output_7.csv',\n",
    "    'modified_output_9.csv',\n",
    "    'modified_output_10.csv',\n",
    "    'modified_output_11.csv',\n",
    "    'modified_output_12.csv',\n",
    "    'modified_output_13.csv',\n",
    "    'modified_output_pose1.csv',\n",
    "    'modified_output_pose2.csv',\n",
    "    'modified_output_pose3.csv',\n",
    "    'modified_output_pose4.csv',\n",
    "    'modified_output_pose5.csv'\n",
    "]\n",
    "\n",
    "# Step 2: 각 CSV 파일을 읽어 DataFrame 리스트로 저장\n",
    "df_list = [pd.read_csv(path) for path in file_paths]\n",
    "\n",
    "# Step 3: 리스트의 모든 DataFrame을 하나로 연결\n",
    "modify_df = pd.concat(df_list, ignore_index=True)\n",
    "modify_df = modify_df.dropna()\n",
    "# Step 4: 결과 확인 또는 저장\n",
    "\n",
    "cols_to_check = ['field.pose_array.poses0.position.x', 'field.pose_array.poses0.position.y',\"field.pose_array.poses0.position.z\"]\n",
    "dup_mask = modify_df.duplicated(subset=cols_to_check, keep=False)\n",
    "\n",
    "# 3) Keep only rows that are unique in the specified columns\n",
    "modify_df = modify_df[~dup_mask].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c66fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_euler_with_fixed_ref(q_ref: np.ndarray,\n",
    "                                 q_targets: np.ndarray):\n",
    "    \"\"\"\n",
    "    Compute roll, pitch, yaw of each q_target relative to a fixed q_ref.\n",
    "    \n",
    "    Parameters:\n",
    "      q_ref      : array of shape (4,), [w, x, y, z]\n",
    "      q_targets  : array of shape (N,4), [w, x, y, z] per row\n",
    "    \n",
    "    Returns:\n",
    "      roll       : array of shape (N,), rotation about X-axis (radians)\n",
    "      pitch      : array of shape (N,), rotation about Y-axis (radians)\n",
    "      yaw        : array of shape (N,), rotation about Z-axis (radians)\n",
    "    \"\"\"\n",
    "    # 1) Normalize reference quaternion\n",
    "    q1 = q_ref.astype(float)\n",
    "    q1 /= np.linalg.norm(q1)\n",
    "    \n",
    "    # 2) Normalize all target quaternions\n",
    "    norms = np.linalg.norm(q_targets, axis=1, keepdims=True)\n",
    "    q2 = q_targets.astype(float) / norms\n",
    "\n",
    "    # 3) Conjugate of q_ref: (w, -x, -y, -z)\n",
    "    q1_conj = q1.copy()\n",
    "    q1_conj[1:] *= -1  # invert vector part\n",
    "\n",
    "    # 4) Broadcast q1_conj to shape (N,4) for multiplication\n",
    "    w1, x1, y1, z1 = q1_conj\n",
    "    w2, x2, y2, z2 = q2.T\n",
    "\n",
    "    # quaternion multiplication q_rel = q1_conj * q2\n",
    "    w_rel =  w1*w2 - x1*x2 - y1*y2 - z1*z2\n",
    "    x_rel =  w1*x2 + x1*w2 + y1*z2 - z1*y2\n",
    "    y_rel =  w1*y2 - x1*z2 + y1*w2 + z1*x2\n",
    "    z_rel =  w1*z2 + x1*y2 - y1*x2 + z1*w2\n",
    "\n",
    "    # 5) Convert to Euler angles\n",
    "    roll  = np.arctan2(2*(w_rel*x_rel + y_rel*z_rel),\n",
    "                       1 - 2*(x_rel**2 + y_rel**2))\n",
    "    pitch = np.arcsin(2*(w_rel*y_rel - z_rel*x_rel))\n",
    "    yaw   = np.arctan2(2*(w_rel*z_rel + x_rel*y_rel),\n",
    "                       1 - 2*(y_rel**2 + z_rel**2))\n",
    "\n",
    "    return roll, pitch, yaw\n",
    "\n",
    "def add_euler_columns(df: 'pd.DataFrame',\n",
    "                      q_ref: np.ndarray,\n",
    "                      target_cols: list,\n",
    "                      col_names: list = ['roll', 'pitch', 'yaw']\n",
    "                     ) -> 'pd.DataFrame':\n",
    "    \"\"\"\n",
    "    Compute roll, pitch, yaw angles for each quaternion in df[target_cols]\n",
    "    relative to a fixed reference quaternion q_ref, and assign them as new columns.\n",
    "\n",
    "    Parameters:\n",
    "      df         : pd.DataFrame containing quaternion data\n",
    "      q_ref      : np.ndarray of shape (4,), [w, x, y, z]\n",
    "      target_cols: list of 4 column names in df representing [w, x, y, z]\n",
    "      col_names  : list of 3 names for the new Euler angle columns\n",
    "\n",
    "    Returns:\n",
    "      df_out     : a new DataFrame with the added Euler columns\n",
    "    \"\"\"\n",
    "    # 1) Extract target quaternions as NumPy array\n",
    "    q_targets = df[target_cols].to_numpy()  # shape (N,4)\n",
    "\n",
    "    # 2) Compute Euler angles relative to q_ref\n",
    "    roll_vals, pitch_vals, yaw_vals = compute_euler_with_fixed_ref(q_ref, q_targets)\n",
    "\n",
    "    # 3) Assign to new columns in a copy of the DataFrame\n",
    "    df_out = df.copy()\n",
    "    df_out[col_names[0]] = roll_vals\n",
    "    df_out[col_names[1]] = pitch_vals\n",
    "    df_out[col_names[2]] = yaw_vals\n",
    "\n",
    "    return df_out\n",
    "# ===== Usage Example =====\n",
    "\n",
    "# suppose df is your DataFrame with these columns:\n",
    "# ['ref_w','ref_x','ref_y','ref_z','tgt_w','tgt_x','tgt_y','tgt_z']\n",
    "import pandas as pd\n",
    "df = modify_df\n",
    "\n",
    "# extract as numpy arrays\n",
    "# Suppose df is your DataFrame, q_ref is defined\n",
    "target_cols = ['field.pose_array.poses0.orientation.w',\n",
    "               'field.pose_array.poses0.orientation.x',\n",
    "               'field.pose_array.poses0.orientation.y',\n",
    "               'field.pose_array.poses0.orientation.z']\n",
    "q_ref = np.array([0, 0, -np.sqrt(0.5), np.sqrt(0.5)])\n",
    "\n",
    "df_with_euler = add_euler_columns(df, q_ref, target_cols,col_names=['field.angles8', 'field.angles9', 'field.angles10'])\n",
    "print(df_with_euler[['field.angles8', 'field.angles9', 'field.angles10']].head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b29c9ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "\n",
    "# Step 1: 파일 경로 패턴 설정 (예: 'data/file1.csv' ~ 'data/file8.csv')\n",
    "file_paths = [\n",
    "    'modified_output_AA1.csv',\n",
    "    'modified_output_AA2.csv',\n",
    "    'modified_output_AA3.csv',\n",
    "    'modified_output_AA4.csv',\n",
    "    'modified_output_AA5.csv',\n",
    "    'modified_output_AA6.csv',\n",
    "    'modified_output_AA7.csv',\n",
    "    'modified_output_AA8.csv',\n",
    "    'modified_output_AA9.csv',\n",
    "    'modified_output_AA10.csv'\n",
    "]\n",
    "\n",
    "# Step 2: 각 CSV 파일을 읽어 DataFrame 리스트로 저장\n",
    "df_list = [pd.read_csv(path) for path in file_paths]\n",
    "\n",
    "# Step 3: 리스트의 모든 DataFrame을 하나로 연결\n",
    "merged_df = pd.concat(df_list, ignore_index=True)\n",
    "merged_df = merged_df.dropna()\n",
    "# Step 4: 결과 확인 또는 저장\n",
    "print(merged_df.shape)   # (총 행 수, 열 수)\n",
    "print(merged_df.head())  # 상위 5개 행 미리보기\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17fe5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_check = ['field.pose_array.poses0.position.x', 'field.pose_array.poses0.position.y',\"field.pose_array.poses0.position.z\"]\n",
    "dup_mask = merged_df.duplicated(subset=cols_to_check, keep=False)\n",
    "\n",
    "# 3) Keep only rows that are unique in the specified columns\n",
    "merged_df = merged_df[~dup_mask].reset_index(drop=True)\n",
    "\n",
    "# df_merged = pd.concat([df_with_euler, merged_df], axis=0, ignore_index=True)\n",
    "# df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189e3521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# List of keywords to remove\n",
    "keywords = ['orientation', 'poses0', 'poses21', 'poses22', 'poses23', 'poses24', 'poses25']\n",
    "pattern = '|'.join(keywords)\n",
    "\n",
    "# 1) Find columns containing any of the keywords\n",
    "# mask_any_keyword = df_merged.columns.str.contains(pattern)\n",
    "mask_any_keyword = merged_df.columns.str.contains(pattern)\n",
    "\n",
    "# 2) Identify columns that contain both 'orientation' and 'poses0'\n",
    "# mask_both = (\n",
    "#     df_merged.columns.str.contains('orientation') &\n",
    "#     df_merged.columns.str.contains('poses0')\n",
    "# )\n",
    "\n",
    "mask_both = (\n",
    "    merged_df.columns.str.contains('orientation') &\n",
    "    merged_df.columns.str.contains('poses0')\n",
    ")\n",
    "\n",
    "# 3) Build final mask:\n",
    "#    - Keep if NOT containing any keyword, OR\n",
    "#    - Keep if containing both 'orientation' and 'poses0'\n",
    "final_mask = ~mask_any_keyword \n",
    "\n",
    "# 4) Apply mask to DataFrame\n",
    "# df_merged = df_merged.loc[:, final_mask]\n",
    "df_merged = merged_df.loc[:, final_mask]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41aa36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = df_merged.columns.tolist()\n",
    "new_cols = (\n",
    "    cols[:60] +\n",
    "    cols[68:71] +\n",
    "    cols[60:68] +    \n",
    "    [cols[71]]\n",
    ")\n",
    "df_merged = df_merged[new_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9933c80b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = df_merged[df_merged['field.angles1'] >= -25]\n",
    "df_merged = df_merged[df_merged['field.angles2'] >= -25]\n",
    "df_merged = df_merged[df_merged['field.angles3'] >= -25]\n",
    "\n",
    "df_merged = df_merged[df_merged['field.angles1'] <= 25]\n",
    "df_merged = df_merged[df_merged['field.angles2'] <= 25]\n",
    "df_merged = df_merged[df_merged['field.angles3'] <= 25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9570819",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "df_merged.columns\n",
    "X = df_merged.iloc[:, :-9].values         \n",
    "y = df_merged.iloc[:, -8:-5].values          \n",
    "\n",
    "\n",
    "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "y_tensor = torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "X.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c385d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def remove_rows_with_outliers(\n",
    "    y_tensor: torch.Tensor,\n",
    "    X: torch.Tensor,\n",
    "    window_size: int = 31,\n",
    "    abs_diff_thresh: float = 10.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Detect outliers in each column of y_tensor by\n",
    "      |y[t,c] - local_window_mean[t,c]| > abs_diff_thresh,\n",
    "    then remove any row t containing an outlier in ANY column,\n",
    "    from BOTH y_tensor and X.\n",
    "\n",
    "    Parameters:\n",
    "      y_tensor        : torch.Tensor of shape [T, C]\n",
    "      X               : torch.Tensor of shape [T, ...] (same first dim T)\n",
    "      window_size     : odd int, size of the rolling-mean window\n",
    "      abs_diff_thresh : float, absolute difference threshold\n",
    "\n",
    "    Returns:\n",
    "      clean_y         : torch.Tensor [T - num_removed, C]\n",
    "      clean_X         : torch.Tensor [T - num_removed, ...]\n",
    "      removed_indices : 1D LongTensor of removed row indices\n",
    "    \"\"\"\n",
    "    assert y_tensor.dim() == 2 and X.dim() >= 2\n",
    "    T, C = y_tensor.shape\n",
    "    assert X.size(0) == T, \"X must have same number of rows as y_tensor\"\n",
    "    assert window_size % 2 == 1, \"window_size must be odd\"\n",
    "\n",
    "    pad = window_size // 2\n",
    "    kernel = torch.ones(window_size, device=y_tensor.device) / window_size\n",
    "    kernel = kernel.view(1, 1, -1)\n",
    "\n",
    "    # 마스크 초기화\n",
    "    remove_mask = torch.zeros(T, dtype=torch.bool, device=y_tensor.device)\n",
    "\n",
    "    # 각 컬럼마다 rolling mean 계산 후 이상치 마킹\n",
    "    for c in range(C):\n",
    "        ts = y_tensor[:, c].view(1,1,T)\n",
    "        ts_padded = F.pad(ts, (pad, pad), mode='reflect')\n",
    "        rolling_mean = F.conv1d(ts_padded, kernel).view(-1)\n",
    "        diff = (y_tensor[:, c] - rolling_mean).abs()\n",
    "        remove_mask |= (diff > abs_diff_thresh)\n",
    "\n",
    "    removed_indices = remove_mask.nonzero(as_tuple=True)[0]\n",
    "\n",
    "    # 행 제거\n",
    "    clean_y = y_tensor[~remove_mask]\n",
    "    clean_X = X[~remove_mask]\n",
    "\n",
    "    return clean_y, clean_X, removed_indices\n",
    "\n",
    "\n",
    "# ===== Usage Example =====\n",
    "# y_tensor: [T, 3], X: [T, D]\n",
    "clean_y, clean_X, removed_idx = remove_rows_with_outliers(\n",
    "    y_tensor,\n",
    "    X_tensor,\n",
    "    window_size=401,\n",
    "    abs_diff_thresh=5.0\n",
    ")\n",
    "\n",
    "print(\"Removed row indices:\", removed_idx.tolist())\n",
    "print(\"Original shapes:\", y_tensor.shape, X.shape)\n",
    "print(\"Cleaned shapes: \", clean_y.shape, clean_X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae03172",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save((clean_X, clean_y), 'dataset.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2129fbe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Step 1: 데이터 불러오기 및 중복 제거\n",
    "test_df = pd.read_csv(\"modified_output_AA_test.csv\")\n",
    "test_df = test_df.dropna()\n",
    "cols_to_check = ['field.pose_array.poses0.position.x', 'field.pose_array.poses0.position.y',\"field.pose_array.poses0.position.z\"]\n",
    "dup_mask = test_df.duplicated(subset=cols_to_check, keep=False)\n",
    "\n",
    "test_df = add_euler_columns(test_df, q_ref, target_cols,col_names=['field.angles8', 'field.angles9', 'field.angles10'])\n",
    "\n",
    "# 3) Keep only rows that are unique in the specified columns\n",
    "test_df = test_df[~dup_mask].reset_index(drop=True)\n",
    "test_df = test_df[new_cols]\n",
    "\n",
    "import re\n",
    "\n",
    "# List of keywords to remove\n",
    "keywords = ['orientation', 'poses0', 'poses21', 'poses22', 'poses23', 'poses24', 'poses25']\n",
    "pattern = '|'.join(keywords)\n",
    "\n",
    "# 1) Find columns containing any of the keywords\n",
    "mask_any_keyword = test_df.columns.str.contains(pattern)\n",
    "\n",
    "# 2) Identify columns that contain both 'orientation' and 'poses0'\n",
    "mask_both = (\n",
    "    test_df.columns.str.contains('orientation') &\n",
    "    test_df.columns.str.contains('poses0')\n",
    ")\n",
    "\n",
    "# 3) Build final mask:\n",
    "#    - Keep if NOT containing any keyword, OR\n",
    "#    - Keep if containing both 'orientation' and 'poses0'\n",
    "final_mask = ~mask_any_keyword \n",
    "\n",
    "# 4) Apply mask to DataFrame\n",
    "test_df = test_df.loc[:, final_mask]\n",
    "test_df = test_df[new_cols]\n",
    "\n",
    "# Step 2: X, y 분리\n",
    "X_test = torch.tensor(test_df.iloc[:, :-9].values, dtype=torch.float32)\n",
    "y = test_df.iloc[:, -8:-5].values.astype(float)\n",
    "\n",
    "# Step 3: 값이 160보다 큰 경우 부호 반전\n",
    "# y = np.where(y_raw > 160, -y_raw, y_raw)\n",
    "\n",
    "# Step 4: 저장된 스케일러 불러오기\n",
    "# scaler = joblib.load('y_scaler.pkl')\n",
    "\n",
    "# Step 5: y에 스케일러 적용\n",
    "# y_scaled = scaler.transform(y)\n",
    "\n",
    "# Step 6: 텐서로 변환 후 저장\n",
    "y_test = torch.tensor(y, dtype=torch.float32)\n",
    "torch.save((X_test, y_test), 'test.pt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a427f63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_locations = test_df.isna()\n",
    "\n",
    "# Get list of positions (row index, column name) where NaN exists\n",
    "nan_positions = [(row_idx, col) for row_idx, row in nan_locations.iterrows() for col, is_nan in row.items() if is_nan]\n",
    "\n",
    "# Print the results\n",
    "if nan_positions:\n",
    "    print(f'Total NaN count: {len(nan_positions)}')\n",
    "    print('NaN locations:')\n",
    "    for pos in nan_positions:\n",
    "        print(f'Row index: {pos[0]}, Column: {pos[1]}')\n",
    "else:\n",
    "    print('There are no NaN values in the DataFrame.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27248dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "idx=1\n",
    "# Plot histogram as normalized (probability density)\n",
    "plt.hist(y_tensor[:, idx], bins=20, edgecolor='black', alpha=0.6, density=True, label='merged_df')\n",
    "plt.hist(y_test[:, idx], bins=20, edgecolor='black', alpha=0.6, density=True, label='test_df')\n",
    "\n",
    "plt.title('Normalized Histogram of Angle')\n",
    "plt.xlabel('Age')\n",
    "plt.ylabel('Density')  # not Frequency, since we're using density=True\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fbc7aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(2,1,1)\n",
    "plt.plot(y_tensor[:,0])\n",
    "plt.subplot(2,1,2)\n",
    "plt.plot(clean_y[:,0])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200e2d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Replace with your actual .npz path\n",
    "npz_path = 'model_weights.npz'\n",
    "\n",
    "try:\n",
    "    data = np.load(npz_path)\n",
    "    print(f\"Loaded .npz file: {npz_path}\")\n",
    "    print(f\"Found {len(data.files)} arrays:\\n\")\n",
    "\n",
    "    for key in data.files:\n",
    "        arr = data[key]\n",
    "        shape = arr.shape\n",
    "        ndim = arr.ndim\n",
    "\n",
    "        # Heuristic classification:\n",
    "        if ndim == 2:\n",
    "            # 2D weight: likely Linear.weight\n",
    "            # shape = (out_features, in_features)\n",
    "            print(f\"  - {key}: shape={shape}, dtype={arr.dtype} => likely Linear.weight\")\n",
    "        elif ndim == 1:\n",
    "            # 1D: could be LayerNorm.weight or LayerNorm.bias or Linear.bias.\n",
    "            # 추가적으로 이름으로 유추:\n",
    "            if key.endswith('.weight'):\n",
    "                # 1D weight: if preceding layer is LayerNorm(prev_dim), then this is LayerNorm.weight.\n",
    "                print(f\"  - {key}: shape={shape}, dtype={arr.dtype} => 1D weight: likely LayerNorm.weight\")\n",
    "            elif key.endswith('.bias'):\n",
    "                # 1D bias: could be LayerNorm.bias or Linear.bias.\n",
    "                # 이름만으로 구분이 어려우므로, 앞의 인덱스와 shape를 보고 추론 필요.\n",
    "                # 예: ffn.0.bias shape=(182,)인데, ffn.0.weight도 (182,)이므로 LayerNorm.bias.\n",
    "                #     ffn.1.bias shape=(512,)인데 weight shape (512,182)이면 Linear.bias.\n",
    "                # 간단히 두 가지 모두 가능하다고 표시:\n",
    "                print(f\"  - {key}: shape={shape}, dtype={arr.dtype} => 1D bias: LayerNorm.bias or Linear.bias\")\n",
    "            else:\n",
    "                print(f\"  - {key}: shape={shape}, dtype={arr.dtype} => 1D array: unknown role\")\n",
    "        else:\n",
    "            print(f\"  - {key}: shape={shape}, dtype={arr.dtype} => unexpected ndim={ndim}\")\n",
    "\n",
    "    # 추가로, 1D bias가 Linear.bias인지 LayerNorm.bias인지 정확히 구분하려면:\n",
    "    print(\"\\nAdditional check for 1D bias vs LayerNorm.bias:\")\n",
    "    for key in data.files:\n",
    "        if key.endswith('.bias') and data[key].ndim == 1:\n",
    "            idx = key.split('.')[1]  # 'ffn.<idx>.bias'\n",
    "            weight_key = f\"ffn.{idx}.weight\"\n",
    "            if weight_key in data.files:\n",
    "                w = data[weight_key]\n",
    "                if w.ndim == 1 and w.shape == data[key].shape:\n",
    "                    # weight도 1D, bias shape 같음 → LayerNorm.bias\n",
    "                    role = \"LayerNorm.bias (matched 1D weight)\"\n",
    "                elif w.ndim == 2 and w.shape[0] == data[key].shape[0]:\n",
    "                    # weight is 2D with out_features equal bias length → Linear.bias\n",
    "                    role = \"Linear.bias (matched 2D weight out_features)\"\n",
    "                else:\n",
    "                    role = \"Unknown bias role\"\n",
    "            else:\n",
    "                role = \"No matching weight key; unknown\"\n",
    "            print(f\"  - {key}: shape={data[key].shape} => {role}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"File not found: {npz_path}. Please ensure the file exists or update the path.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading or processing the npz file: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddb840f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "from model import  Skeleton2Mesh\n",
    "# 2) Load test data\n",
    "X_test, y_test = torch.load('test.pt',weights_only=False)\n",
    "\n",
    "# 1) Device 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 3) Hyperparameters / model config\n",
    "num_joints    = 20\n",
    "num_bones     = 19\n",
    "gsd_dim       = 100\n",
    "mlp_hidden    = [256, 256]\n",
    "pe_freqs_bone = 5\n",
    "pe_freqs_order= 2\n",
    "\n",
    "# 4) Instantiate model and load checkpoint\n",
    "model = Skeleton2Mesh(\n",
    "    num_joints=num_joints,\n",
    "    num_bones=num_bones,\n",
    "    gsd_dim=gsd_dim,\n",
    "    mlp_hidden=mlp_hidden,\n",
    "    pe_freqs_bone=pe_freqs_bone,\n",
    "    pe_freqs_order=pe_freqs_order\n",
    ").to(device)\n",
    "\n",
    "# Path to saved checkpoint\n",
    "checkpoint_path = \"checkpoint_euler_epoch1000_ver2.pth\"  # 실제 파일명으로 변경\n",
    "# checkpoint_path = \"checkpoint_epoch900.pth\"  # 실제 파일명으로 변경\n",
    "\n",
    "# Load checkpoint\n",
    "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "model.eval()  # set to evaluation mode\n",
    "\n",
    "# If optimizer state or epoch 등 추가 정보가 필요하면\n",
    "# epoch_loaded = checkpoint.get('epoch')\n",
    "# loss_loaded = checkpoint.get('loss')\n",
    "\n",
    "# 5) Prepare test data\n",
    "#    Assume you have NumPy arrays:\n",
    "#      skeletons_test_np: shape [N_test, num_joints*3]\n",
    "#      angles_test_np:    shape [N_test, 8]\n",
    "#    These must be prepared beforehand.\n",
    "import numpy as np\n",
    "# Example placeholders; 실제 데이터를 여기에 할당하세요.\n",
    "# skeletons_test_np = np.load(\"skeletons_test.npy\")  # [N_test, 63]\n",
    "# angles_test_np    = np.load(\"angles_test.npy\")     # [N_test, 8]\n",
    "\n",
    "# Convert to torch.Tensor\n",
    "\n",
    "# Create DataLoader\n",
    "batch_size = 64  # 필요에 따라 변경\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# 6) Define evaluation metrics\n",
    "mse_loss_fn = nn.MSELoss(reduction='mean')\n",
    "mae_loss_fn = nn.L1Loss(reduction='mean')\n",
    "\n",
    "# Optional: per-dimension metrics\n",
    "num_outputs = y_test.shape[1]  # e.g. 8\n",
    "\n",
    "# Containers for aggregated results\n",
    "total_mse = 0.0\n",
    "total_mae = 0.0\n",
    "num_samples = 0\n",
    "\n",
    "# If you want per-output errors, accumulate sums:\n",
    "sum_sq_errors = torch.zeros(num_outputs, device=device)\n",
    "sum_abs_errors = torch.zeros(num_outputs, device=device)\n",
    "\n",
    "# 7) Evaluation loop\n",
    "with torch.no_grad():\n",
    "    for skeletons_flat_batch, angles_batch in test_loader:\n",
    "        skeletons_flat_batch = skeletons_flat_batch.to(device)  # [B, num_joints*3]\n",
    "        angles_batch         = angles_batch.to(device)         # [B, 8]\n",
    "\n",
    "        # Forward pass\n",
    "        preds = model(skeletons_flat_batch)  # [B, 8]\n",
    "\n",
    "        # Compute batch losses\n",
    "        batch_mse = mse_loss_fn(preds, angles_batch)  # scalar\n",
    "        batch_mae = mae_loss_fn(preds, angles_batch)  # scalar\n",
    "\n",
    "        # Accumulate weighted by batch size\n",
    "        B = skeletons_flat_batch.size(0)\n",
    "        total_mse += batch_mse.item() * B\n",
    "        total_mae += batch_mae.item() * B\n",
    "        num_samples += B\n",
    "\n",
    "        # Per-output accumulation\n",
    "        # Compute squared error and abs error per sample per dimension\n",
    "        diff = preds - angles_batch  # [B, 8]\n",
    "        sum_sq_errors += torch.sum(diff * diff, dim=0)    # [8]\n",
    "        sum_abs_errors += torch.sum(torch.abs(diff), dim=0)  # [8]\n",
    "\n",
    "# 8) Final metrics\n",
    "mean_mse = total_mse / num_samples\n",
    "mean_mae = total_mae / num_samples\n",
    "\n",
    "# Per-output RMSE and MAE\n",
    "rmse_per_output = torch.sqrt(sum_sq_errors / num_samples)  # [8]\n",
    "mae_per_output  = sum_abs_errors / num_samples            # [8]\n",
    "\n",
    "print(f\"Test MSE (mean over all outputs): {mean_mse:.6f}\")\n",
    "print(f\"Test MAE (mean over all outputs): {mean_mae:.6f}\")\n",
    "print(\"Per-output RMSE:\", rmse_per_output.cpu().numpy())\n",
    "print(\"Per-output MAE: \", mae_per_output.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a77c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원하는 인덱스를 지정\n",
    "index = 791 # 원하는 인덱스로 변경 가능\n",
    "\n",
    "# 1) 입력 데이터 추출 및 모델로 예측\n",
    "x_sample = X_test[index].unsqueeze(0).to(device)  # [1, input_dim]\n",
    "y_true   = y_test[index].cpu().numpy()            # [output_dim]\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred = model(x_sample).cpu().numpy().flatten()  # [output_dim]\n",
    "\n",
    "# 2) 결과 출력\n",
    "print(f\"Index: {index}\")\n",
    "print(\"Predicted:\", y_pred)\n",
    "print(\"Ground Truth:\", y_true)\n",
    "\n",
    "# 3) Optional: 차이 출력\n",
    "print(\"Absolute Error:\", np.abs(y_pred - y_true))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b47dee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure model is in eval mode\n",
    "model.eval()\n",
    "\n",
    "# 1) 예측값과 true값을 모두 저장할 리스트 초기화\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x_batch, y_batch in test_loader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        preds = model(x_batch)\n",
    "\n",
    "        all_preds.append(preds.cpu())\n",
    "        all_targets.append(y_batch.cpu())\n",
    "\n",
    "# 2) 모든 예측값과 실제값을 연결 (concat)\n",
    "all_preds_tensor = torch.cat(all_preds, dim=0)       # shape [N, output_dim]\n",
    "all_targets_tensor = torch.cat(all_targets, dim=0)   # shape [N, output_dim]\n",
    "\n",
    "# 3) 샘플별 MSE 계산\n",
    "mse_per_sample = torch.mean((all_preds_tensor - all_targets_tensor) ** 2, dim=1)  # shape [N]\n",
    "\n",
    "# 4) 최대 MSE 인덱스 탐색\n",
    "max_mse_value, max_mse_index = torch.max(mse_per_sample, dim=0)\n",
    "\n",
    "# 5) 해당 인덱스의 예측값과 실제값\n",
    "y_pred_max = all_preds_tensor[max_mse_index].numpy()\n",
    "y_true_max = all_targets_tensor[max_mse_index].numpy()\n",
    "\n",
    "# 6) 출력\n",
    "print(f\"Index with Maximum MSE: {max_mse_index.item()}\")\n",
    "print(f\"Maximum MSE value: {max_mse_value.item():.6f}\")\n",
    "print(\"Predicted   :\", y_pred_max)\n",
    "print(\"Ground Truth:\", y_true_max)\n",
    "print(\"Abs Error   :\", np.abs(y_pred_max - y_true_max))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560dd420",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "# ---------------------------\n",
    "# 1. EMA 필터 함수\n",
    "# ---------------------------\n",
    "def apply_ema(tensor: torch.Tensor, alpha: float = 0.1) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply EMA (Exponential Moving Average) to each column of a 2D tensor.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame(tensor.numpy())\n",
    "    ema_df = df.ewm(alpha=alpha, adjust=False).mean()\n",
    "    return torch.tensor(ema_df.values, dtype=tensor.dtype)\n",
    "\n",
    "# ---------------------------\n",
    "# 2. Butterworth 필터 함수\n",
    "# ---------------------------\n",
    "def apply_butterworth(tensor: torch.Tensor, cutoff: float, fs: float, order: int = 2) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Apply Butterworth low-pass filter to each column of a 2D tensor.\n",
    "    \"\"\"\n",
    "    b, a = butter(N=order, Wn=cutoff / (fs / 2), btype='low')\n",
    "    tensor_np = tensor.numpy()\n",
    "    filtered_np = np.zeros_like(tensor_np)\n",
    "    \n",
    "    for i in range(tensor_np.shape[1]):\n",
    "        filtered_np[:, i] = lfilter(b, a, tensor_np[:, i])\n",
    "    \n",
    "    return torch.tensor(filtered_np, dtype=tensor.dtype)\n",
    "\n",
    "# ---------------------------\n",
    "# 3. 필터 적용\n",
    "# ---------------------------\n",
    "ema_preds_tensor     = apply_ema(all_preds_tensor, alpha=0.5)\n",
    "butter_preds_tensor  = apply_butterworth(all_preds_tensor, cutoff=10.0, fs=60.0, order=2)\n",
    "all_filter_preds_tensor     = apply_ema(butter_preds_tensor, alpha=0.5)\n",
    "# butter_preds_tensor = ema_preds_tensor\n",
    "\n",
    "# ---------------------------\n",
    "# 4. 시각화\n",
    "# ---------------------------\n",
    "plt.figure(figsize=(10,12))\n",
    "\n",
    "plt.subplot(4,1,1)\n",
    "plt.plot(mse_per_sample.numpy(), marker='o', linestyle='-', markersize=2)\n",
    "plt.title('MSE per Sample Index')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('MSE')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "\n",
    "mse_ema_sample = torch.mean((ema_preds_tensor - all_targets_tensor) ** 2, dim=1)  # shape [N]\n",
    "\n",
    "plt.subplot(5,1,2)\n",
    "plt.plot(mse_ema_sample.numpy(), marker='o', linestyle='-', markersize=2)\n",
    "\n",
    "prev_butter = butter_preds_tensor[0,:]\n",
    "prev_ema = ema_preds_tensor[0,:]\n",
    "prev_all = all_filter_preds_tensor[0,:]\n",
    "\n",
    "for i in range(1,len(butter_preds_tensor)):\n",
    "    butter_preds_tensor[i,:] = np.clip(butter_preds_tensor[i,:],-0.15 + prev_butter,0.15+ prev_butter)\n",
    "    ema_preds_tensor[i,:] = np.clip(ema_preds_tensor[i,:],-0.15 + prev_ema,0.15+ prev_ema)\n",
    "    all_filter_preds_tensor[i,:] = np.clip(all_filter_preds_tensor[i,:],-0.15 + prev_all,0.15+ prev_all)\n",
    "    prev_butter = butter_preds_tensor[i,:]\n",
    "    prev_ema = ema_preds_tensor[i,:]\n",
    "    prev_all = all_filter_preds_tensor[i,:]\n",
    "\n",
    "for i in range(2,5):\n",
    "    plt.subplot(5,1,i+1)\n",
    "    plt.plot(all_preds_tensor[:, i-2], label=\"pred\", alpha=0.4)\n",
    "    plt.plot(ema_preds_tensor[:, i-2], label=\"pred (EMA)\", linewidth=1.8)\n",
    "    plt.plot(butter_preds_tensor[:, i-2], label=\"pred (Butter)\", linewidth=1.8)\n",
    "    plt.plot(all_filter_preds_tensor[:, i-2], label=\"pred (all)\", linewidth=1.8)\n",
    "    plt.plot(all_targets_tensor[:, i-2], label=\"target\", linestyle='--')\n",
    "    plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809fbcb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 4))\n",
    "plt.hist(mse_per_sample.numpy(), bins=50, edgecolor='black')\n",
    "plt.title('Histogram of MSE per Sample')\n",
    "plt.xlabel('MSE Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d974fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1,11):\n",
    "# Path to saved checkpoint\n",
    "    checkpoint_path = f\"checkpoint_euler_epoch{i*100}.pth\"  # 실제 파일명으로 변경\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "    print(checkpoint['loss'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb2f71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor, y_tensor = torch.load('dataset.pt')  # X: [N,182], y: [N]\n",
    "X_tensor.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hand",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
